{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://example.com/taxonfinder/config.schema.json",
  "title": "TaxonFinder Config",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "confidence",
    "locale"
  ],
  "properties": {
    "confidence": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "description": "Minimum extraction_confidence threshold for including a result in the output."
    },
    "locale": {
      "type": "string",
      "minLength": 2,
      "description": "Locale preference for taxon common names in iNaturalist API."
    },
    "gazetteer_path": {
      "type": "string",
      "minLength": 1,
      "default": "data/gazetteer.db",
      "description": "Path to the gazetteer SQLite database (relative to project root)."
    },
    "llm_extractor": {
      "type": ["object", "null"],
      "description": "LLM extractor settings (Phase 1). Set to null or omit to disable.",
      "additionalProperties": false,
      "required": ["provider", "model"],
      "properties": {
        "enabled": {
          "type": "boolean",
          "default": true,
          "description": "Whether the LLM extractor is enabled."
        },
        "provider": {
          "type": "string",
          "enum": ["ollama", "openai", "anthropic"],
          "description": "LLM provider."
        },
        "model": {
          "type": "string",
          "minLength": 1,
          "description": "Model name for the chosen provider (e.g. llama3.1, gpt-4o-mini)."
        },
        "url": {
          "type": "string",
          "format": "uri",
          "description": "URL for LLM connection. Required for Ollama. Optional for cloud providers."
        },
        "timeout": {
          "type": "number",
          "minimum": 1,
          "maximum": 600,
          "default": 60,
          "description": "Timeout in seconds for LLM responses."
        },
        "prompt_file": {
          "type": "string",
          "minLength": 1,
          "default": "prompts/llm_extractor.txt",
          "description": "Path to the LLM extractor prompt file (relative to project root)."
        },
        "chunk_strategy": {
          "type": "string",
          "enum": ["paragraph", "page"],
          "default": "paragraph",
          "description": "Strategy for splitting text into chunks."
        },
        "min_chunk_words": {
          "type": "integer",
          "minimum": 10,
          "maximum": 500,
          "default": 50,
          "description": "Minimum chunk size in words. Smaller chunks are merged with adjacent ones."
        },
        "max_chunk_words": {
          "type": "integer",
          "minimum": 100,
          "maximum": 2000,
          "default": 500,
          "description": "Maximum chunk size in words. Larger chunks are split by sentence boundaries."
        }
      }
    },
    "llm_enricher": {
      "type": ["object", "null"],
      "description": "LLM enricher settings (Phase 4: enrichment of unresolved candidates). Set to null or omit to disable.",
      "additionalProperties": false,
      "required": ["provider", "model"],
      "properties": {
        "enabled": {
          "type": "boolean",
          "default": true,
          "description": "Whether the LLM enricher is enabled."
        },
        "provider": {
          "type": "string",
          "enum": ["ollama", "openai", "anthropic"],
          "description": "LLM provider."
        },
        "model": {
          "type": "string",
          "minLength": 1,
          "description": "Model name for the chosen provider."
        },
        "url": {
          "type": "string",
          "format": "uri",
          "description": "URL for LLM connection. Required for Ollama. Optional for cloud providers."
        },
        "timeout": {
          "type": "number",
          "minimum": 1,
          "maximum": 300,
          "default": 30,
          "description": "Timeout in seconds for LLM responses."
        },
        "prompt_file": {
          "type": "string",
          "minLength": 1,
          "default": "prompts/llm_enricher.txt",
          "description": "Path to the LLM enricher prompt file (relative to project root)."
        }
      }
    }
  }
}
