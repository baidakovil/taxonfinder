{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://example.com/taxonfinder/config.schema.json",
  "title": "TaxonFinder Config",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "confidence",
    "locale",
    "llm_provider",
    "llm_model"
  ],
  "properties": {
    "confidence": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "description": "Minimum extraction_confidence threshold for including a result in the output."
    },
    "locale": {
      "type": "string",
      "minLength": 2,
      "description": "Locale preference for taxon common names in iNaturalist API."
    },
    "llm_prompt_file": {
      "type": "string",
      "minLength": 1,
      "default": "prompts/taxon_extraction.txt",
      "description": "Path to the LLM prompt template file (relative to project root)."
    },
    "llm_provider": {
      "type": "string",
      "enum": [
        "ollama",
        "openai",
        "anthropic"
      ],
      "description": "LLM provider to use."
    },
    "llm_model": {
      "type": "string",
      "minLength": 1,
      "description": "Model name for the chosen provider (e.g. llama3.1, gemma2, gpt-4o-mini)."
    },
    "llm_url": {
      "type": "string",
      "format": "uri",
      "description": "URL for LLM connection. Required for Ollama (e.g. http://localhost:11434). Optional for cloud providers."
    },
    "llm_timeout": {
      "type": "number",
      "minimum": 1,
      "maximum": 300,
      "default": 30,
      "description": "Timeout in seconds for LLM responses."
    }
  }
}
