{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://example.com/taxonfinder/config.schema.json",
  "title": "TaxonFinder Config",
  "type": "object",
  "additionalProperties": false,
  "required": [
    "confidence",
    "locale"
  ],
  "properties": {
    "confidence": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "description": "Minimum extraction_confidence threshold for including a result in the output."
    },
    "locale": {
      "type": "string",
      "minLength": 2,
      "description": "Locale preference for taxon common names in iNaturalist API."
    },
    "gazetteer_path": {
      "type": "string",
      "minLength": 1,
      "default": "data/gazetteer.db",
      "description": "Path to the gazetteer SQLite database (relative to project root)."
    },
    "spacy_model": {
      "type": "string",
      "minLength": 1,
      "default": "ru_core_news_md",
      "description": "Name of the spaCy language model to use for tokenization and sentence segmentation."
    },
    "max_file_size_mb": {
      "type": "number",
      "minimum": 0.1,
      "maximum": 100,
      "default": 2.0,
      "description": "Maximum input file size in megabytes. Files exceeding this limit are rejected before processing."
    },
    "degraded_mode": {
      "type": "boolean",
      "default": false,
      "description": "When true, missing gazetteer is a WARNING (pipeline continues with available extractors). When false (default), missing gazetteer is a fatal error."
    },
    "user_agent": {
      "type": "string",
      "minLength": 1,
      "default": "TaxonFinder/0.1.0",
      "description": "User-Agent header for all outgoing HTTP requests (iNaturalist API, LLM providers). Required by iNaturalist API guidelines."
    },
    "inaturalist": {
      "type": "object",
      "additionalProperties": false,
      "description": "iNaturalist API connection and caching settings.",
      "properties": {
        "base_url": {
          "type": "string",
          "format": "uri",
          "default": "https://api.inaturalist.org",
          "description": "Base URL for iNaturalist API. Override for proxy or mock in tests."
        },
        "timeout": {
          "type": "number",
          "minimum": 1,
          "maximum": 120,
          "default": 30,
          "description": "Total request timeout in seconds."
        },
        "rate_limit": {
          "type": "number",
          "minimum": 0.1,
          "maximum": 10,
          "default": 1.0,
          "description": "Sustained request rate (requests per second)."
        },
        "burst_limit": {
          "type": "integer",
          "minimum": 1,
          "maximum": 20,
          "default": 5,
          "description": "Maximum burst size for token bucket rate limiter."
        },
        "max_retries": {
          "type": "integer",
          "minimum": 0,
          "maximum": 10,
          "default": 3,
          "description": "Maximum retry attempts on 429/5xx errors."
        },
        "cache_enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable disk cache for API responses."
        },
        "cache_path": {
          "type": "string",
          "minLength": 1,
          "default": "cache/taxonfinder.db",
          "description": "Path to the disk cache SQLite database."
        },
        "cache_ttl_days": {
          "type": "integer",
          "minimum": 1,
          "maximum": 365,
          "default": 7,
          "description": "Time-to-live for cached API responses in days."
        }
      }
    },
    "llm_extractor": {
      "type": ["object", "null"],
      "description": "LLM extractor settings (Phase 1). Set to null or omit to disable.",
      "additionalProperties": false,
      "required": ["provider", "model"],
      "properties": {
        "enabled": {
          "type": "boolean",
          "default": true,
          "description": "Whether the LLM extractor is enabled."
        },
        "provider": {
          "type": "string",
          "enum": ["ollama", "openai", "anthropic"],
          "description": "LLM provider."
        },
        "model": {
          "type": "string",
          "minLength": 1,
          "description": "Model name for the chosen provider (e.g. llama3.1, gpt-4o-mini)."
        },
        "url": {
          "type": "string",
          "format": "uri",
          "description": "URL for LLM connection. Required for Ollama. Optional for cloud providers."
        },
        "timeout": {
          "type": "number",
          "minimum": 1,
          "maximum": 600,
          "default": 60,
          "description": "Timeout in seconds for LLM responses."
        },
        "prompt_file": {
          "type": "string",
          "minLength": 1,
          "default": "prompts/llm_extractor.txt",
          "description": "Path to the LLM extractor prompt file (relative to project root)."
        },
        "chunk_strategy": {
          "type": "string",
          "enum": ["paragraph", "page"],
          "default": "paragraph",
          "description": "Strategy for splitting text into chunks."
        },
        "min_chunk_words": {
          "type": "integer",
          "minimum": 10,
          "maximum": 500,
          "default": 50,
          "description": "Minimum chunk size in words. Smaller chunks are merged with adjacent ones."
        },
        "max_chunk_words": {
          "type": "integer",
          "minimum": 100,
          "maximum": 2000,
          "default": 500,
          "description": "Maximum chunk size in words. Larger chunks are split by sentence boundaries."
        }
      }
    },
    "llm_enricher": {
      "type": ["object", "null"],
      "description": "LLM enricher settings (Phase 4: enrichment of unresolved candidates). Set to null or omit to disable.",
      "additionalProperties": false,
      "required": ["provider", "model"],
      "properties": {
        "enabled": {
          "type": "boolean",
          "default": true,
          "description": "Whether the LLM enricher is enabled."
        },
        "provider": {
          "type": "string",
          "enum": ["ollama", "openai", "anthropic"],
          "description": "LLM provider."
        },
        "model": {
          "type": "string",
          "minLength": 1,
          "description": "Model name for the chosen provider."
        },
        "url": {
          "type": "string",
          "format": "uri",
          "description": "URL for LLM connection. Required for Ollama. Optional for cloud providers."
        },
        "timeout": {
          "type": "number",
          "minimum": 1,
          "maximum": 300,
          "default": 30,
          "description": "Timeout in seconds for LLM responses."
        },
        "prompt_file": {
          "type": "string",
          "minLength": 1,
          "default": "prompts/llm_enricher.txt",
          "description": "Path to the LLM enricher prompt file (relative to project root)."
        }
      }
    }
  }
}
